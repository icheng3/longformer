{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4JnoCUoL-2Jz",
        "outputId": "536b8631-c231-4003-fc14-c4150bf0b092"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# crash colab to get more RAM\n",
        "# !kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "o9IkphgF-90-",
        "outputId": "b10ad656-2a05-420f-80a6-544d1c57d784"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Dec 11 17:41:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Pzh-JNaUK3Y_",
        "outputId": "00d53458-d360-41a2-9365-c00f1824fe19"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install datasets==2.7.1\n",
        "!pip install transformers==4.25.1\n",
        "!pip install tensorflow\n",
        "!pip install rouge_score\n",
        "!pip install -U datasets transformers ninja -q\n",
        "!pip install -U sentencepiece -q\n",
        "!pip install clean-text[gpl] -q\n",
        "!pip install from_pandas\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vGz6j0pVM6hF",
        "outputId": "39df772e-83ef-4ca3-cae2-054cc6c2e0c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ODLQ8MUJfmi4",
        "outputId": "921304ad-e1d2-4a0d-e63a-8b9645904a03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "# adding auto-Colab formatting with `IPython.display`\n",
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "da3f31170f1f4000a2167fa31d7a9fba",
            "3d33ab7971094c6780496d915ae3c919",
            "a120ea6447b0405aaf5734e5c0c2207f",
            "3132df509bb14081a3094b70f69cc0fe",
            "f1ff8237afd845f3b184e164afc4aba5",
            "2a3b5619967a4baa9786f87bc7eee365",
            "0b99075048c1457081b919743250e417",
            "87e502ff68e54b0590eee18baf246026",
            "1b0a4fa4e8d744e6a68dcc14dacb7833",
            "267d25c90073429da139fb4cee8553fa",
            "964baa10d5d044e4929f84a02f615df2"
          ]
        },
        "id": "VmjgpyOAHYUP",
        "outputId": "48de8ce6-4c92-4c01-f172-3eadef951413"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration icheng3--book_dataset_1470-1a060ad188199dd1\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/icheng3___csv/icheng3--book_dataset_1470-1a060ad188199dd1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da3f31170f1f4000a2167fa31d7a9fba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"icheng3/book_dataset_1470\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317,
          "referenced_widgets": [
            "e2fb3e7be4144ce1b3e90da034cf1c15",
            "b42e86fd94f645d79439a511b6638021",
            "190bd36f1c3c4c4b83f8db24859b049e",
            "7516c19d41de4f6cb2c8279f6fbed2dc",
            "97b640e57a9b48c5ba374e86144d33c3",
            "321329a721ae42a9b235fa15144a9ffe",
            "41306568c3ef4365ac4a3945b5a6421f",
            "983cccbc68244182891aa5d01143862c",
            "4aff8586e7744934b0211374812de22a",
            "c31e59f233ab4cf3b9fc29e278e38868",
            "8d5846b7f44a4c2b8db37440135b257d"
          ]
        },
        "id": "V0ra9Hk1IGd0",
        "outputId": "1419c741-e608-42a6-cc4b-78e7f55c6c34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration icheng3--book_dataset_1470-1a060ad188199dd1\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/icheng3___csv/icheng3--book_dataset_1470-1a060ad188199dd1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2fb3e7be4144ce1b3e90da034cf1c15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/icheng3___csv/icheng3--book_dataset_1470-1a060ad188199dd1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a1403ab3e270dbb8.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 1192\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Features, Value, ClassLabel\n",
        "from transformers import RobertaTokenizer\n",
        "from datasets import Dataset \n",
        "import pandas as pd\n",
        "\n",
        "def tokenization(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\"\") \n",
        "\n",
        "dataset = load_dataset('icheng3/book_dataset_1470')\n",
        "\n",
        "\n",
        "dataset = dataset.map(tokenization,batched=True, batch_size=2)\n",
        "dataset.set_format(type=\"tf\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "print(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "w4dxvSSlXHf5",
        "outputId": "b5399e3d-633f-4ad6-f1fb-6f11727253ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" RoBERTa configuration\"\"\"\n",
        "from collections import OrderedDict\n",
        "from typing import Mapping, Union, List\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.onnx import OnnxConfig\n",
        "from transformers.utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    \"roberta-base\": \"https://huggingface.co/roberta-base/resolve/main/config.json\",\n",
        "    \"roberta-large\": \"https://huggingface.co/roberta-large/resolve/main/config.json\",\n",
        "    \"roberta-large-mnli\": \"https://huggingface.co/roberta-large-mnli/resolve/main/config.json\",\n",
        "    \"distilroberta-base\": \"https://huggingface.co/distilroberta-base/resolve/main/config.json\",\n",
        "    \"roberta-base-openai-detector\": \"https://huggingface.co/roberta-base-openai-detector/resolve/main/config.json\",\n",
        "    \"roberta-large-openai-detector\": \"https://huggingface.co/roberta-large-openai-detector/resolve/main/config.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class RobertaConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "    This is the configuration class to store the configuration of a [`RobertaModel`] or a [`TFRobertaModel`]. It is\n",
        "    used to instantiate a RoBERTa model according to the specified arguments, defining the model architecture.\n",
        "    Instantiating a configuration with the defaults will yield a similar configuration to that of the RoBERTa\n",
        "    [roberta-base](https://huggingface.co/roberta-base) architecture.\n",
        "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
        "    documentation from [`PretrainedConfig`] for more information.\n",
        "    Args:\n",
        "        vocab_size (`int`, *optional*, defaults to 30522):\n",
        "            Vocabulary size of the RoBERTa model. Defines the number of different tokens that can be represented by the\n",
        "            `inputs_ids` passed when calling [`RobertaModel`] or [`TFRobertaModel`].\n",
        "        hidden_size (`int`, *optional*, defaults to 768):\n",
        "            Dimensionality of the encoder layers and the pooler layer.\n",
        "        num_hidden_layers (`int`, *optional*, defaults to 12):\n",
        "            Number of hidden layers in the Transformer encoder.\n",
        "        num_attention_heads (`int`, *optional*, defaults to 12):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        intermediate_size (`int`, *optional*, defaults to 3072):\n",
        "            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
        "        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
        "            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n",
        "        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "        max_position_embeddings (`int`, *optional*, defaults to 512):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        type_vocab_size (`int`, *optional*, defaults to 2):\n",
        "            The vocabulary size of the `token_type_ids` passed when calling [`RobertaModel`] or [`TFRobertaModel`].\n",
        "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n",
        "            The epsilon used by the layer normalization layers.\n",
        "        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n",
        "            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n",
        "            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n",
        "            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n",
        "            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n",
        "            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n",
        "        is_decoder (`bool`, *optional*, defaults to `False`):\n",
        "            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n",
        "        use_cache (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
        "            relevant if `config.is_decoder=True`.\n",
        "        classifier_dropout (`float`, *optional*):\n",
        "            The dropout ratio for the classification head.\n",
        "    Examples:\n",
        "    ```python\n",
        "    >>> from transformers import RobertaConfig, RobertaModel\n",
        "    >>> # Initializing a RoBERTa configuration\n",
        "    >>> configuration = RobertaConfig()\n",
        "    >>> # Initializing a model (with random weights) from the configuration\n",
        "    >>> model = RobertaModel(configuration)\n",
        "    >>> # Accessing the model configuration\n",
        "    >>> configuration = model.config\n",
        "    ```\"\"\"\n",
        "    model_type = \"roberta\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_window: Union[List[int], int] = 512,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=1,\n",
        "        bos_token_id=0,\n",
        "        eos_token_id=2,\n",
        "        position_embedding_type=\"absolute\",\n",
        "        use_cache=True,\n",
        "        classifier_dropout=None,\n",
        "        attention_dilation=1,\n",
        "        attention_mode = 'sliding_chunks',\n",
        "        autoregressive = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.position_embedding_type = position_embedding_type\n",
        "        self.use_cache = use_cache\n",
        "        self.classifier_dropout = classifier_dropout\n",
        "        self.attention_window = attention_window\n",
        "        self.attention_dilation = attention_dilation\n",
        "        self.attention_mode = attention_mode\n",
        "        self.autoregressive = autoregressive\n",
        "\n",
        "\n",
        "class RobertaOnnxConfig(OnnxConfig):\n",
        "    @property\n",
        "    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n",
        "        if self.task == \"multiple-choice\":\n",
        "            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n",
        "        else:\n",
        "            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n",
        "        return OrderedDict(\n",
        "            [\n",
        "                (\"input_ids\", dynamic_axis),\n",
        "                (\"attention_mask\", dynamic_axis),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "n40AmbbUP41D",
        "outputId": "0c16a0ba-967c-4981-b8d5-003c74d9cac7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 1192\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "_QMRAn-NOyK0",
        "outputId": "ca78752f-36f3-42c4-f37e-b87bb9bceae5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-454b307fd51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "print(tf_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dJLcf13uRGOb",
        "outputId": "529912d9-baf7-4c04-d114-492b5974ccd8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model.\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sliding_chunks import sliding_chunks_matmul_pv\n",
        "\n",
        "\n",
        "from transformers.modeling_tf_utils import (\n",
        "    TFMaskedLanguageModelingLoss,\n",
        "    TFModelInputType,\n",
        "    TFMultipleChoiceLoss,\n",
        "    TFPreTrainedModel,\n",
        "    TFMultipleChoiceLoss,\n",
        "    get_initializer,\n",
        "    unpack_inputs,\n",
        ")\n",
        "\n",
        "from transformers.modeling_tf_outputs import (\n",
        "TFMultipleChoiceModelOutput,\n",
        " TFBaseModelOutputWithPastAndCrossAttentions,\n",
        " TFMaskedLMOutput,\n",
        ")\n",
        "\n",
        "from transformers.tf_utils import shape_list\n",
        "\n",
        "from transformers.utils import (\n",
        "    MULTIPLE_CHOICE_DUMMY_INPUTS,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        ")\n",
        "from roberta_config import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.word_embeddings = self.add_weight(\n",
        "                name=\"weight\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                initializer=get_initializer(config.initializer_range),\n",
        "            )\n",
        "        self.position_embeddings = self.add_weight(\n",
        "                name=\"embeddings\",\n",
        "                shape=[config.max_position_embeddings, config.hidden_size],\n",
        "                initializer=get_initializer(config.initializer_range),\n",
        "            )\n",
        "        self.token_type_embeddings = self.add_weight(\n",
        "                name=\"embeddings\",\n",
        "                shape=[config.type_vocab_size, config.hidden_size],\n",
        "                initializer=get_initializer(config.initializer_range),\n",
        "            )\n",
        "        self.LayerNorm = self.LayerNormalization(epsilon=config.layer_norm_eps, name=\"Layer_norm\")\n",
        "        self.dropout = self.Dropout(config.hidden_dropout_prob)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "    \n",
        "    def create_position_ids_from_input_ids(self, input_ids, past_key_values_length=0):\n",
        "        mask = tf.cast(tf.math.not_equal(input_ids, self.padding_idx), dtype=input_ids.dtype)\n",
        "        incremental_indices = (tf.math.cumsum(mask, axis=1) + past_key_values_length) * mask\n",
        "        return incremental_indices + self.padding_idx\n",
        "    def create_position_ids_from_input_embeds(self, input_ids, past_key_values_length=0):\n",
        "        input_shape = input_ids.get_shape().as_list()\n",
        "        position_ids = tf.expand_dims(\n",
        "                    tf.range(start=self.padding_idx + 1, limit=input_shape[-1] + self.padding_idx + 1, dtype=tf.int64),\n",
        "                    axis=0,\n",
        "                )\n",
        "        return position_ids\n",
        "\n",
        "    def call(\n",
        "        self, \n",
        "        input_ids=None, \n",
        "        token_type_ids=None, \n",
        "        position_ids=None, \n",
        "        inputs_embeds=None, \n",
        "        past_key_values_length=0, \n",
        "        training=False\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids\n",
        "                position_ids = self.create_position_ids_from_input_ids(input_ids, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n",
        "        else:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.zeros(input_shape, dtype=tf.int64)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        \n",
        "        token_type_embeddings = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        position_embeddings = tf.gather(params=self.position_embeddings, indices=position_ids)\n",
        "        embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class RobertaSelfOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = self.Dense(\n",
        "            units=config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = self.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = self.Dropout(rate=config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor, training:bool=False):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaAttention():\n",
        "    def __init__(self, config, layer_id=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.self = LongformerSelfAttention(config, layer_id, name='roberta_self_att')\n",
        "        self.output = RobertaSelfOutput(config, name='roberta_output')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        (\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions\n",
        "        ) = inputs\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            [hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions], training=training\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states, training=training)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "    \n",
        "\n",
        "class RobertaIntermediate(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = self.Dense(config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range,\n",
        "        name='dense_layer'))\n",
        "        self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = self.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range,\n",
        "        name='dense_layer'))\n",
        "        self.LayerNorm = self.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layernorm\")\n",
        "        self.dropout = self.Dropout(rate=config.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, hidden_states, input_tensor, training: bool=False):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "class RobertaLayer():\n",
        "    def __init__(self, config, layer_id=0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config, layer_id, name='attention')\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        self.intermediate = RobertaIntermediate(config, name='intermed')\n",
        "        self.output = RobertaOutput(config, name='output')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \n",
        "        (\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            output_attentions,\n",
        "        ) = inputs\n",
        "       \n",
        "        self_attention_outputs = self.attention(\n",
        "            [hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions], training=True\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "        \n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(\n",
        "            intermediate_output, attention_output,training=training \n",
        "        )\n",
        "        outputs = (layer_output,) + attention_output[1:]\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class RobertaEncoder():\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.config = config\n",
        "        self.layer = [RobertaLayer(config, i, name=f\"layer_._{i}\") for i in range(config.num_hidden_layers)]\n",
        "        self.gradient_checkpointing = False\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask,\n",
        "        head_mask,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        past_key_values,\n",
        "        output_attentions,\n",
        "        output_hidden_states,\n",
        "        return_dict,\n",
        "        padding_len=0,\n",
        "        use_cache=False,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                hidden_states_to_add = hidden_states[:, :-padding_len] if padding_len > 0 else hidden_states\n",
        "                all_hidden_states = all_hidden_states + (hidden_states_to_add,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "        \n",
        "        if output_hidden_states:\n",
        "            hidden_states_to_add = hidden_states[:, :-padding_len] if padding_len > 0 else hidden_states\n",
        "            all_hidden_states = all_hidden_states + (hidden_states_to_add,)\n",
        "        hidden_states = hidden_states[:, :-padding_len] if padding_len > 0 else hidden_states\n",
        "        if output_attentions:\n",
        "            all_attentions = (\n",
        "                tuple([state[:, :, :-padding_len, :] for state in all_attentions])\n",
        "                if padding_len > 0\n",
        "                else all_attentions\n",
        "            )\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return TFBaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = self.Dense(\n",
        "            units=config.hidden_size,\n",
        "            kernel_initializer=get_initializer(config.initializer_range),\n",
        "            name=\"pooler_dense\",\n",
        "        )\n",
        "        self.activation = tf.keras.activations.tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output                                                                                                         \n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(TFPreTrainedModel): #COME BACK TO THIS\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = []\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, tf.keras.layers.Dense):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            initializer = tf.keras.initializers.RandomNormal(mean=0.0, std=self.config.initializer_range)\n",
        "            module.weight.data = initializer(tf.shape(module.weight.data))\n",
        "            # module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                # module.bias.data.zero_()\n",
        "                tf.zeros(tf.shape(module.bias.data), dtype=tf.dtypes.float32)\n",
        "        elif isinstance(module, tf.keras.layers.Embedding):\n",
        "            initializer = tf.keras.initializers.RandomNormal(mean=0.0, std=self.config.initializer_range)\n",
        "            module.weight.data = initializer(tf.shape(module.weight.data))\n",
        "            # module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx] = 0\n",
        "        elif isinstance(module, tf.keras.layers.LayerNorm):\n",
        "            tf.ones(tf.shape(module.bias.data), dtype=tf.dtypes.float32)\n",
        "            #module.weight.data.fill_(1.0)\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, RobertaEncoder):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "    Parameters:\n",
        "        config ([`RobertaConfig`]): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using [`RobertaTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n",
        "            - 0 corresponds to a *sentence A* token,\n",
        "            - 1 corresponds to a *sentence B* token.\n",
        "            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n",
        "            >= 2. All the value in this tensor should be always < type_vocab_size.\n",
        "            [What are token type IDs?](../glossary#token-type-ids)\n",
        "        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
        "            config.max_position_embeddings - 1]`.\n",
        "            [What are position IDs?](../glossary#position-ids)\n",
        "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
        "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
        "            model's internal embedding lookup matrix.\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(TFPreTrainedModel): \n",
        "    \"\"\"\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
        "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
        "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
        "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
        "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "    config_class = RobertaConfig\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "        config.attention_window = [config.attention_window] * config.num_hidden_layers \n",
        "        self.config = config\n",
        "        self.num_hidden_layers = config.num_hidden_layers\n",
        "        self.initializer_range = config.initializer_range\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.return_dict = config.use_return_dict\n",
        "        self.pad_token_id = config.pad_token_id\n",
        "        self.attention_window = config.attention_window\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TFBaseModelOutputWithPastAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def call(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        position_ids,\n",
        "        head_mask,\n",
        "        inputs_embeds,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        past_key_values,\n",
        "        output_attentions,\n",
        "        output_hidden_states,\n",
        "        return_dict,\n",
        "        training=False,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        \"\"\"\n",
        "        ##let's first make sure all the right types...\n",
        "        input_ids = tf.cast(input_ids, dtype=tf.dtypes.int64)\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "        if attention_mask is None:\n",
        "            attention_mask = tf.cast(tf.fill(input_shape, 1), tf.int64)\n",
        "        attention_mask = tf.cast(attention_mask, tf.int64)\n",
        "\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            tf.cast(tf.fill(input_shape, 0), tf.int64)\n",
        "\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "        attention_mask_shape = shape_list(attention_mask)\n",
        "        extended_attention_mask = tf.reshape(attention_mask, (attention_mask_shape[0], attention_mask_shape[1], 1, 1))\n",
        "        encoder_extended_attention_mask = tf.cast(tf.math.abs(1 - extended_attention_mask), tf.dtypes.float32) * -10000.0\n",
        "        \n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "            training=training\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "         \n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return TFBaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions, #may need to set to none\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top.\"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(TFPreTrainedModel, TFMaskedLanguageModelingLoss): \n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False, name='roberta_model')\n",
        "        self.lm_head = RobertaLMHead(config, self.roberta.embeddings, name='lm_head')\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TFMaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "        expected_output=\"' Paris'\",\n",
        "        expected_loss=0.1,\n",
        "    )\n",
        "    def call(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        position_ids,\n",
        "        head_mask,\n",
        "        inputs_embeds,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        labels,\n",
        "        output_attentions,\n",
        "        output_hidden_states,\n",
        "        return_dict,\n",
        "        training: Optional[bool] = False,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
        "            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
        "            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
        "        kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            training= training,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output, training=training)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            masked_lm_loss = self.hf_compute_loss(labels, prediction_scores)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return TFMaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            #global_attention=outputs.global_attentions,\n",
        "        )\n",
        "\n",
        "class RobertaLMHead(tf.keras.layers.Layer):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = self.Dense(\n",
        "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.layer_norm = self.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n",
        "\n",
        "        self.decoder = input_embeddings\n",
        "        self.bias = self.add_weight(shape=(config.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden = self.dense(hidden_states)\n",
        "        hidden = tf.keras.activations.gelu(hidden)\n",
        "        hidden = self.layer_norm(hidden)\n",
        "\n",
        "        seq_length = shape_list(tensor=hidden)[1]\n",
        "        hidden = tf.reshape(tensor=hidden, shape=[-1, self.hidden_size])\n",
        "        hidden = tf.matmul(a=hidden, b=self.decoder.weight, transpose_b=True)\n",
        "        hidden = tf.reshape(tensor=hidden, shape=[-1, seq_length, self.vocab_size])\n",
        "        hidden = tf.nn.bias_add(value=hidden, bias=self.bias)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel, TFMultipleChoiceLoss, tf.keras.layers.Layer):\n",
        "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"lm_head\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"dropout\"]\n",
        "\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = self.Dense(\n",
        "            1, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        \"\"\"\n",
        "        Dummy inputs to build the network.\n",
        "        Returns:\n",
        "            tf.Tensor with dummy inputs\n",
        "        \"\"\"\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int32)}\n",
        "\n",
        "    @unpack_inputs\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TFMultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def call(\n",
        "        self,\n",
        "        input_ids: Optional[TFModelInputType] = None,\n",
        "        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
        "        training: Optional[bool] = False,\n",
        "    ) -> Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]:\n",
        "        r\"\"\"\n",
        "        labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n",
        "            where `num_choices` is the size of the second dimension of the input tensors. (See `input_ids` above)\n",
        "        \"\"\"\n",
        "\n",
        "        if input_ids is not None:\n",
        "            num_choices = shape_list(input_ids)[1]\n",
        "            seq_length = shape_list(input_ids)[2]\n",
        "        else:\n",
        "            num_choices = shape_list(inputs_embeds)[1]\n",
        "            seq_length = shape_list(inputs_embeds)[2]\n",
        "\n",
        "        flat_input_ids = tf.reshape(input_ids, (-1, seq_length)) if input_ids is not None else None\n",
        "        flat_attention_mask = tf.reshape(attention_mask, (-1, seq_length)) if attention_mask is not None else None\n",
        "        flat_token_type_ids = tf.reshape(token_type_ids, (-1, seq_length)) if token_type_ids is not None else None\n",
        "        flat_position_ids = tf.reshape(position_ids, (-1, seq_length)) if position_ids is not None else None\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            flat_attention_mask,\n",
        "            flat_token_type_ids,\n",
        "            flat_position_ids,\n",
        "            head_mask,\n",
        "            inputs_embeds,\n",
        "            output_attentions,\n",
        "            output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            training=training,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\n",
        "\n",
        "        loss = None if labels is None else self.hf_compute_loss(labels, reshaped_logits)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TFMultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=[\n",
        "            {\n",
        "                \"input_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"input_ids\"),\n",
        "                \"attention_mask\": tf.TensorSpec((None, None, None), tf.int32, name=\"attention_mask\"),\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    def serving(self, inputs):\n",
        "        output = self.call(inputs)\n",
        "\n",
        "        return self.serving_output(output)\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForMultipleChoice.serving_output\n",
        "    def serving_output(self, output: TFMultipleChoiceModelOutput) -> TFMultipleChoiceModelOutput:\n",
        "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\n",
        "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\n",
        "\n",
        "        return TFMultipleChoiceModelOutput(logits=output.logits, hidden_states=hs, attentions=attns)\n",
        "\n",
        "class LongformerSelfAttention(tf.keras.layers.Layer): #used to be nn.module\n",
        "    def __init__(self, config, layer_id, **kwargs):\n",
        "        super(LongformerSelfAttention, self).__init__(**kwargs)\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.query = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.key = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.value = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.query_global = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.key_global = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.value_global = self.Dense(self.embed_dim, activation='relu')\n",
        "        self.dropout = config.attention_probs_dropout_prob \n",
        "        self.layer_id = layer_id\n",
        "        self.attention_window = config.attention_window\n",
        "        self.attention_dilation = config.attention_dilation\n",
        "        self.attention_mode = config.attention_mode \n",
        "        self.autoregressive = config.autoregressive \n",
        "        assert self.attention_window > 0\n",
        "        assert self.attention_dilation > 0\n",
        "        assert self.attention_mode in ['tvm', 'sliding_chunks', 'sliding_chunks_no_overlap']\n",
        "        if self.attention_mode in ['sliding_chunks', 'sliding_chunks_no_overlap']:\n",
        "            assert not self.autoregressive  # not supported\n",
        "            assert self.attention_dilation == 1  # dilation is not supported\n",
        "\n",
        "    def call(self, inputs, training=False,):\n",
        "        (\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            output_attentions,\n",
        "        ) = inputs\n",
        "        '''\n",
        "        The `attention_mask` is changed in `BertModel.forward` from 0, 1, 2 to\n",
        "            -ve: no attention\n",
        "              0: local attention\n",
        "            +ve: global attention\n",
        "        '''\n",
        "        assert encoder_hidden_states is None, \"`encoder_hidden_states` is not supported and should be None\"\n",
        "        assert encoder_attention_mask is None, \"`encoder_attention_mask` is not supported and shiould be None\"\n",
        "        assert head_mask is None, \"`head_mask` is not supported and should be None\"\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            #attention_mask = attention_mask.squeeze(dim=2).squeeze(dim=1)\n",
        "            attention_mask = tf.squeeze(tf.squeeze(attention_mask, axis=2), axis=1)\n",
        "            key_padding_mask = attention_mask < 0\n",
        "            extra_attention_mask = attention_mask > 0\n",
        "            remove_from_windowed_attention_mask = attention_mask != 0\n",
        "\n",
        "            num_extra_indices_per_batch = tf.math.reduce_sum(tf.convert_to_tensor(extra_attention_mask, dtype=tf.int64)\n",
        "    , axis=1, name='num_extra_indices_per_batch')\n",
        "            extra_attention_mask = None\n",
        "\n",
        "        else:\n",
        "            remove_from_windowed_attention_mask = None\n",
        "            extra_attention_mask = None\n",
        "            key_padding_mask = None\n",
        "\n",
        "        hidden_states = tf.transpose(hidden_states)\n",
        "        seq_len, bsz, embed_dim = tf.TensorShape(hidden_states).as_list()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        q = self.query(hidden_states)\n",
        "        k = self.key(hidden_states)\n",
        "        v = self.value(hidden_states)\n",
        "        q /= math.sqrt(tf.cast(self.head_dim, dtype=q.dtype))\n",
        "\n",
        "        q = tf.reshape(q, (bsz, seq_len, self.num_heads, self.head_dim))\n",
        "        k = tf.reshape(k, (bsz, seq_len, self.num_heads, self.head_dim))\n",
        "        # attn_weights = (bsz, seq_len, num_heads, window*2+1)\n",
        "        attn_weights = sliding_chunks_matmul_qk(q, k, self.attention_window, padding_value=0)\n",
        "        if remove_from_windowed_attention_mask is not None:\n",
        "            # This implementation is fast and takes very little memory because num_heads x hidden_size = 1\n",
        "            # from (bsz x seq_len) to (bsz x seq_len x num_heads x hidden_size)\n",
        "            remove_from_windowed_attention_mask = tf.expand_dims(tf.expand_dims(remove_from_windowed_attention_mask, axis=-1), axis=-1)\n",
        "            # cast to float/half then replace 1's with -inf\n",
        "            float_mask = tf.where(remove_from_windowed_attention_mask,-10000.0, tf.cast(remove_from_windowed_attention_mask, dtype=q.dtype))\n",
        "            repeat_size = 1 if isinstance(self.attention_dilation, int) else len(self.attention_dilation)\n",
        "            # float_mask = float_mask.repeat(1, 1, repeat_size, 1)\n",
        "            float_mask = tf.repeat(float_mask, 1, repeat_size, 1)\n",
        "            ones = float_mask.new_ones(size=float_mask.size())  # tensor of ones\n",
        "            # diagonal mask with zeros everywhere and -inf inplace of padding\n",
        "            d_mask = sliding_chunks_matmul_qk(ones, float_mask, self.attention_window, padding_value=0)\n",
        "\n",
        "            attn_weights += d_mask\n",
        "            attn_probs = tf.keras.activations.softmax(attn_weights, axis=-1)\n",
        "        assert list(attn_weights.size())[:3] == [bsz, seq_len, self.num_heads]\n",
        "        assert attn_weights.size(dim=3) in [self.attention_window * 2 + 1, self.attention_window * 3]\n",
        "        attn_probs = self.dropout(attn_probs, training=training)\n",
        "       \n",
        "        if self.attention_mode == \"sliding_chunks\":#this is the only case we will ever enter i think\n",
        "            attn = sliding_chunks_matmul_pv(attn_probs, v, self.attention_window)\n",
        "        else:\n",
        "            raise False\n",
        "\n",
        "        attn = attn.type_as(hidden_states)\n",
        "        assert list(tf.TensorShape(attn)) == [bsz, seq_len, self.num_heads, self.head_dim]\n",
        "        attn = tf.reshape(attn, (bsz, seq_len, embed_dim))\n",
        "       \n",
        "        outputs = (attn, attn_probs) \n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rVRd9_ZbOq5_",
        "outputId": "4d1d5e6a-6679-4bde-af3c-07b760689d63"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from typing import Union\n",
        "from functools import lru_cache\n",
        "\n",
        "import torch\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DiagonaledMM(torch.autograd.Function):\n",
        "    '''Class to encapsulate tvm code for compiling a diagonal_mm function, in addition to calling\n",
        "    this function from PyTorch\n",
        "    '''\n",
        "\n",
        "    function_dict = {}  # save a list of functions, each has a different set of parameters\n",
        "\n",
        "    @staticmethod\n",
        "    def _compile_function(dtype: str, device: str, b0: int = 4, b1: int = 4, b2: int = 16):\n",
        "        '''Compiles a tvm function that computes diagonal_mm\n",
        "        args:\n",
        "        dtype: str in ['float64', 'float32', 'float16']\n",
        "        device: str in ['cpu' or 'cuda']\n",
        "        b0, b1, b2: size of tensor tiles. Very important for good performance\n",
        "\n",
        "        '''\n",
        "        import tvm  # import the full tvm library here for compilation. Don't import at the top of the file in case we don't need to compile\n",
        "        from tvm.contrib import nvcc\n",
        "        @tvm.register_func\n",
        "        def tvm_callback_cuda_compile(code):\n",
        "            \"\"\"Use nvcc compiler for better perf.\"\"\"\n",
        "            ptx = nvcc.compile_cuda(code, target=\"ptx\", arch='sm_52')  # use old arch for this to work on old GPUs\n",
        "            return ptx\n",
        "\n",
        "        assert dtype in ['float16', 'float32', 'float64']\n",
        "        assert device in ['cpu', 'cuda']\n",
        "        device = None if device == 'cpu' else device\n",
        "        tgt_host=\"llvm\"\n",
        "\n",
        "        b = tvm.var('b')  # batch size\n",
        "        n = tvm.var('n')  # sequence length\n",
        "        h = tvm.var('h')  # number of heads\n",
        "        m = tvm.var('m')  # hidden dimension\n",
        "        w = tvm.var('w')  # window size\n",
        "        w_upper = tvm.var('w_upper')  # window size to the right of the word. Should be `0` or `w`\n",
        "        padding = tvm.var('padding')  # padding\n",
        "        transpose_t1 = tvm.var('transpose_t1')  # t1 should be transposed\n",
        "        t1d3 = tvm.var('t1d3')  # last dimension of t1\n",
        "        t3d3 = tvm.var('t3d3')  # last dimension of t3 (the result tensor)\n",
        "        X = tvm.placeholder((b, n, h, t1d3), name='X', dtype=dtype)  # first tensor\n",
        "        Y = tvm.placeholder((b, n, h, m), name='Y', dtype=dtype)  # second tensor\n",
        "        k = tvm.reduce_axis((0, t1d3), name='k')  # dimension to sum over\n",
        "        D = tvm.placeholder((h), name='D', dtype='int')  # dilation per head\n",
        "        output_shape = (b, n, h, t3d3)  # shape of the result tensor\n",
        "        algorithm = lambda l, i, q, j: tvm.sum(\n",
        "            tvm.if_then_else(\n",
        "                t3d3 == m,  # if output dimension == m, then t1 is diagonaled (FIXME: This breaks if t3d3 == m == t1d3)\n",
        "                tvm.if_then_else(\n",
        "                    transpose_t1 == 0,\n",
        "                    tvm.if_then_else(\n",
        "                        tvm.all(\n",
        "                            i + D[q] * (k - w) >= 0,\n",
        "                            i + D[q] * (k - w) < n,\n",
        "                        ),\n",
        "                        X[l, i, q, k] * Y[l, i + D[q] * (k - w), q, j],  # t1 is diagonaled\n",
        "                        padding\n",
        "                    ),\n",
        "                    tvm.if_then_else(\n",
        "                        tvm.all(\n",
        "                            i + D[q] * (k - w_upper) >= 0,  # `w_upper` to handle the case `autoregressive=True`\n",
        "                            i + D[q] * (k - w_upper) < n,\n",
        "                        ),\n",
        "                        X[l, i + D[q] * (k - w_upper), q, (w_upper + w) - k] * Y[l, i + D[q] * (k - w_upper), q, j],  # # t1 is diagonaled and should be transposed\n",
        "                        padding\n",
        "                    ),\n",
        "                ),\n",
        "                tvm.if_then_else(\n",
        "                    tvm.all(\n",
        "                        i + D[q] * (j - w) >= 0,\n",
        "                        i + D[q] * (j - w) < n,\n",
        "                    ),\n",
        "                    X[l, i, q, k] * Y[l, i + D[q] * (j - w), q, k],  # t1 is not diagonaled, but the output tensor is going to be\n",
        "                    padding\n",
        "                )\n",
        "            ), axis=k)\n",
        "\n",
        "        Z = tvm.compute(output_shape, algorithm, name='Z')  # automatically generate cuda code\n",
        "        s = tvm.create_schedule(Z.op)\n",
        "\n",
        "        print('Lowering: \\n ===================== \\n{}'.format(tvm.lower(s, [X, Y, D], simple_mode=True)))\n",
        "\n",
        "        # split long axis into smaller chunks and assing each one to a separate GPU thread/block\n",
        "        ko, ki = s[Z].split(Z.op.reduce_axis[0], factor=b0)\n",
        "        ZF = s.rfactor(Z, ki)\n",
        "\n",
        "        j_outer, j_inner = s[Z].split(s[Z].op.axis[-1], factor=b1)\n",
        "        i_outer, i_inner = s[Z].split(s[Z].op.axis[1], factor=b2)\n",
        "\n",
        "        s[Z].bind(j_outer, tvm.thread_axis(\"blockIdx.x\"))\n",
        "        s[Z].bind(j_inner, tvm.thread_axis(\"threadIdx.y\"))\n",
        "\n",
        "        s[Z].bind(i_outer, tvm.thread_axis(\"blockIdx.y\"))\n",
        "        s[Z].bind(i_inner, tvm.thread_axis(\"threadIdx.z\"))\n",
        "\n",
        "        tx = tvm.thread_axis(\"threadIdx.x\")\n",
        "        s[Z].bind(s[Z].op.reduce_axis[0], tx)\n",
        "        s[ZF].compute_at(s[Z], s[Z].op.reduce_axis[0])\n",
        "        s[Z].set_store_predicate(tx.var.equal(0))\n",
        "\n",
        "        print('Lowering with GPU splits: \\n ===================== \\n{}'.format(tvm.lower(s, [X, Y, D], simple_mode=True)))\n",
        "\n",
        "        # compiling the automatically generated cuda code\n",
        "        diagonaled_mm = tvm.build(s, [X, Y, Z, D, w, w_upper, padding, transpose_t1, t3d3], target=device, target_host=tgt_host, name='diagonaled_mm')\n",
        "        return diagonaled_mm\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_lib_filename(dtype: str, device: str):\n",
        "        base_filename = 'longformer/lib/lib_diagonaled_mm'\n",
        "        return '{}_{}_{}.so'.format(base_filename, dtype, device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _save_compiled_function(f, dtype: str, device: str):\n",
        "        if not os.path.exists('longformer/lib/'):\n",
        "            os.makedirs('longformer/lib/')\n",
        "        f.export_library(DiagonaledMM._get_lib_filename(dtype, device))\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_compiled_function(dtype: str, device: str):\n",
        "        from tvm.module import load  # this can be the small runtime python library, and doesn't need to be the whole thing\n",
        "        filename = DiagonaledMM._get_lib_filename(dtype, device)\n",
        "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "        potential_dirs = ['../../', '../', './', f'{current_dir}/', f'{current_dir}/../']\n",
        "        for potential_dir in  potential_dirs:\n",
        "            filepath = '{}{}'.format(potential_dir, filename)\n",
        "            if os.path.isfile(filepath):\n",
        "                print('Loading tvm binary from: {}'.format(filepath))\n",
        "                return load(filepath)\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_function(dtype: str, device: str):\n",
        "        '''Loads the function from the disk or compile it'''\n",
        "        # A list of arguments that define the function\n",
        "        args = (dtype, device)\n",
        "        if args not in DiagonaledMM.function_dict:\n",
        "            diagonaled_mm = DiagonaledMM._load_compiled_function(dtype, device)  # try to load from disk\n",
        "            if not diagonaled_mm:\n",
        "                print('Tvm binary not found. Compiling ...')\n",
        "                diagonaled_mm = DiagonaledMM._compile_function(dtype, device)  # compile\n",
        "                DiagonaledMM._save_compiled_function(diagonaled_mm, dtype, device)  # save to disk\n",
        "            # convert the tvm function into a pytorch function\n",
        "            from tvm.contrib import dlpack\n",
        "            diagonaled_mm_pytorch = dlpack.to_pytorch_func(diagonaled_mm)  # wrap it as a pytorch function\n",
        "            # save the function into a dictionary to be reused\n",
        "            DiagonaledMM.function_dict[args] = diagonaled_mm_pytorch  # save it in a dictionary for next time\n",
        "        return DiagonaledMM.function_dict[args]\n",
        "\n",
        "    @staticmethod\n",
        "    def _diagonaled_mm(t1: torch.Tensor, t2: torch.Tensor, w: int, d: Union[torch.Tensor,int],\n",
        "                       is_t1_diagonaled: bool = False, transpose_t1: bool = False, padding: int = 0,\n",
        "                       autoregressive: bool = False):\n",
        "        '''Calls the compiled function after checking the input format. This function is called in three different modes.\n",
        "        t1 x t2 = r ==> t1 and t2 are not diagonaled, but r is. Useful for query x key = attention_scores\n",
        "        t1 x t2 = r ==> t1 is diagonaled, but t2 and r are not. Useful to compuate attantion_scores x value = context\n",
        "        t1 x t2 = r ==> t1 is diagonaled and it should be transposed, but t2 and r are not diagonaled. Useful in some of\n",
        "                            the calculations in the backward pass.\n",
        "        '''\n",
        "        dtype = str(t1.dtype).split('.')[1]\n",
        "        device = t1.device.type\n",
        "        assert len(t1.shape) == 4\n",
        "        assert len(t1.shape) == len(t2.shape)\n",
        "        assert t1.shape[:3] == t2.shape[:3]\n",
        "        if isinstance(d, int):  # if d is an integer, replace it with a tensor of the same length\n",
        "                                # as number of heads, and it is filled with the same dilation value\n",
        "            d = t1.new_full(size=(t1.shape[2],), fill_value=d, dtype=torch.int, requires_grad=False)\n",
        "\n",
        "        assert len(d.shape) == 1\n",
        "        assert d.shape[0] == t1.shape[2]  # number of dilation scores should match number of heads\n",
        "        b = t1.shape[0]  # batch size\n",
        "        n = t1.shape[1]  # sequence length\n",
        "        h = t1.shape[2]  # number of heads\n",
        "        m = t2.shape[3]  # hidden dimension\n",
        "        w_upper = 0 if autoregressive else w\n",
        "        c = w_upper + w + 1  # number of diagonals\n",
        "        if is_t1_diagonaled:\n",
        "            assert t1.shape[3] == c\n",
        "            r = t1.new_empty(b, n, h, m)  # allocate spase for the result tensor\n",
        "        else:\n",
        "            assert not transpose_t1\n",
        "            assert t1.shape[3] == m\n",
        "            r = t1.new_empty(b, n, h, c)  # allocate spase for the result tensor\n",
        "\n",
        "        # gets function from memory, from disk or compiles it from scratch\n",
        "        _diagonaled_mm_function = DiagonaledMM._get_function(dtype=dtype, device=device)\n",
        "\n",
        "        # The last argument to this function is a little hacky. It is the size of the last dimension of the result tensor\n",
        "        # We use it as a proxy to tell if t1_is_diagonaled or not (if t1 is diagonaled, result is not, and vice versa).\n",
        "        # The second reason is that the lambda expression in `_compile_function` is easier to express when the shape\n",
        "        # of the output is known\n",
        "        # This functions computes diagonal_mm then saves the result in `r`\n",
        "        if m == c:\n",
        "            # FIXME\n",
        "            print('Error: the hidden dimension {m} shouldn\\'t match number of diagonals {c}')\n",
        "            assert False\n",
        "        _diagonaled_mm_function(t1, t2, r, d, w, w_upper, padding, transpose_t1, m if is_t1_diagonaled else c)\n",
        "        return r\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_tensors(t):\n",
        "        '''Fix `stride()` information of input tensor. This addresses some inconsistency in stride information in PyTorch.\n",
        "        For a tensor t, if t.size(0) == 1, then the value of t.stride()[0] doesn't matter.\n",
        "        TVM expects this value to be the `product(t.size()[1:])` but PyTorch some times sets it to `t.stride()[1]`.\n",
        "        Here's an example to reporduce this issue:\n",
        "            import torch\n",
        "            print(torch.randn(1, 10).stride())\n",
        "            > (10, 1)\n",
        "            print(torch.randn(10, 1).t().contiguous().stride())\n",
        "            > (1, 1)  # expected it to be (10, 1) as above\n",
        "            print(torch.randn(10, 2).t().contiguous().stride())\n",
        "            > (10, 1) # but gets the expected stride if the first dimension is > 1\n",
        "        '''\n",
        "        assert t.is_contiguous()\n",
        "        t_stride = list(t.stride())\n",
        "        t_size = list(t.size())\n",
        "        # Fix wrong stride information for the first dimension. This occures when batch_size=1\n",
        "        if t_size[0] == 1 and t_stride[0] == t_stride[1]:\n",
        "            # In this case, the stride of the first dimension should be the product\n",
        "            # of the sizes  of all other dimensions\n",
        "            t_stride[0] = t_size[1] * t_size[2] * t_size[3]\n",
        "            t = t.as_strided(size=t_size, stride=t_stride)\n",
        "        return t\n",
        "\n",
        "    min_seq_len = 16  # unexpected output if seq_len < 16\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, t1: torch.Tensor, t2: torch.Tensor, w: int, d: Union[torch.Tensor,int], is_t1_diagonaled: bool = False, padding: int = 0, autoregressive: bool = False) -> torch.Tensor:\n",
        "        '''Compuates diagonal_mm of t1 and t2.\n",
        "        args: \n",
        "        t1: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size|number_of_diagonals).\n",
        "            t1 can be a regular tensor (e.g. `query_layer`) or a diagonaled one (e.g. `attention_scores`)\n",
        "        t2: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size). This is always a non-diagonaled\n",
        "            tensor, e.g. `key_layer` or `value_layer`\n",
        "        w: int = window size; number of attentions on each side of the word\n",
        "        d: torch.Tensor or int = dilation of attentions per attention head. If int, the same dilation value will be used for all\n",
        "            heads. If torch.Tensor, it should be 1D of lenth=number of attention heads\n",
        "        is_t1_diagonaled: is t1 a diagonaled or a regular tensor\n",
        "        padding: the padding value to use when accessing invalid locations. This is mainly useful when the padding\n",
        "            needs to be a very large negative value (to compute softmax of attentions). For other usecases,\n",
        "            please use zero padding.\n",
        "        autoregressive: if true, return only the lower triangle\n",
        "        returns: torch.Tensor = (batch_size, seq_len, num_attention_heads, hidden_size|number_of_diagonals)\n",
        "            if t1 is diagonaed, result is non-diagonaled, and vice versa\n",
        "        '''\n",
        "        batch_size, seq_len, num_attention_heads, hidden_size = t1.size()\n",
        "        assert seq_len >= DiagonaledMM.min_seq_len, 'avoid splitting errors by using seq_len >= {}'.format(DiagonaledMM.min_seq_len)  # FIXME\n",
        "        ctx.save_for_backward(t1, t2)\n",
        "        ctx.w = w\n",
        "        ctx.d = d\n",
        "        ctx.is_t1_diagonaled = is_t1_diagonaled\n",
        "        ctx.autoregressive = autoregressive\n",
        "        t1 = DiagonaledMM._prepare_tensors(t1)\n",
        "        t2 = DiagonaledMM._prepare_tensors(t2)\n",
        "        # output = t1.mm(t2)  # what would have been called if this was a regular matmul\n",
        "        output = DiagonaledMM._diagonaled_mm(t1, t2, w, d, is_t1_diagonaled=is_t1_diagonaled, padding=padding, autoregressive=autoregressive)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        t1, t2 = ctx.saved_tensors\n",
        "        w = ctx.w\n",
        "        d = ctx.d\n",
        "        is_t1_diagonaled = ctx.is_t1_diagonaled\n",
        "        autoregressive = ctx.autoregressive\n",
        "        if not grad_output.is_contiguous():\n",
        "            grad_output = grad_output.contiguous()  # tvm requires all input tensors to be contiguous\n",
        "        grad_output = DiagonaledMM._prepare_tensors(grad_output)\n",
        "        t1 = DiagonaledMM._prepare_tensors(t1)\n",
        "        t2 = DiagonaledMM._prepare_tensors(t2)\n",
        "        # http://cs231n.github.io/optimization-2/\n",
        "        # https://pytorch.org/docs/master/notes/extending.html\n",
        "        # grad_t1 = grad_output.mm(t2)  # what would have been called if this was a regular matmul\n",
        "        grad_t1 = DiagonaledMM._diagonaled_mm(grad_output, t2, w, d, is_t1_diagonaled=not is_t1_diagonaled, autoregressive=autoregressive)\n",
        "        # grad_t2 = grad_output.t().mm(t1)  # or `grad_t2 = t1.t().mm(grad_output).t()` because `(AB)^T = B^TA^T`\n",
        "        if is_t1_diagonaled:\n",
        "            grad_t2 = DiagonaledMM._diagonaled_mm(t1, grad_output, w, d, is_t1_diagonaled=True, transpose_t1=True, autoregressive=autoregressive)\n",
        "        else:\n",
        "            grad_t2 = DiagonaledMM._diagonaled_mm(grad_output, t1, w, d, is_t1_diagonaled=True, transpose_t1=True, autoregressive=autoregressive)\n",
        "        return grad_t1, grad_t2, None, None, None, None, None\n",
        "\n",
        "\n",
        "def _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n",
        "    diagonals_list = []\n",
        "    for j in range(-d * w, d, d):\n",
        "        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
        "        diagonal_mask[:-j] = 1\n",
        "        diagonals_list.append(diagonal_mask)\n",
        "    output = torch.stack(diagonals_list, dim=-1)\n",
        "    return output\n",
        "\n",
        "@lru_cache()\n",
        "def _get_invalid_locations_mask(w: int, d: Union[torch.Tensor,int], autoregressive: bool, device: str):\n",
        "    if isinstance(d, int): #only instance\n",
        "        affected_seq_len = w * d\n",
        "        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d) #torch\n",
        "        mask = mask[None, :, None, :]\n",
        "    else:\n",
        "        affected_seq_len = w * d.max()\n",
        "        head_masks = []\n",
        "        d_list = d.cpu().numpy().tolist()\n",
        "        for d in d_list:\n",
        "            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
        "            head_masks.append(one_head_mask)\n",
        "        mask = torch.stack(head_masks, dim=-2)\n",
        "        mask = mask[None, :, :, :]\n",
        "\n",
        "    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool()#.to(device)\n",
        "    return affected_seq_len, mask.bool(), ending_mask\n",
        "\n",
        "def mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: Union[torch.Tensor, int], autoregressive: bool) -> torch.Tensor:\n",
        "    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n",
        "    seq_len = input_tensor.size(1)\n",
        "    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n",
        "    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n",
        "    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n",
        "    if not autoregressive:\n",
        "        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n",
        "        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n",
        "        ending_input.masked_fill_(ending_mask, -float('inf'))\n",
        "\n",
        "\n",
        "# The non-tvm implementation is the default, we don't need to load the kernel at loading time.\n",
        "# DiagonaledMM._get_function('float32', 'cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lY7mQWPsXqSh",
        "outputId": "34cdad3a-61fa-4aaa-ddac-77f303ce5b0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "from diagonaled_mm_tvm import mask_invalid_locations\n",
        "\n",
        "def _skew(x, padding_value):\n",
        "    '''Convert diagonals into columns'''\n",
        "    pads = tf.convert_to_tensor([0,0],[0,0],[0,1],[0,0])\n",
        "    x_padded = tf.pad(x, pads, constant_values=padding_value)\n",
        "    \n",
        "    x_padded = tf.reshape(x_padded, (tf.shape_list(x_padded)[0], tf.shape_list(x_padded)[1], tf.shape_list(x_padded)[-1], tf.shape_list(x_padded)[-2]))\n",
        "    return x_padded\n",
        "\n",
        "\n",
        "def _skew2(x, padding_value):\n",
        "    '''shift every row 1 step to right converting columns into diagonals'''\n",
        "    # X = B x C x M x L\n",
        "    B, C, M, L = tf.shape_list(x)\n",
        "    pads = tf.convert_to_tensor([0,0], [0,0],[0,0],[0, M+1])\n",
        "    x = tf.pad(x, pads, constant_values=padding_value)\n",
        "    x = tf.reshape(x, (B, C, -1))\n",
        "    x = x[:,:,:-M]\n",
        "    x = tf.reshape(x, (B, C, M, M + L))\n",
        "    x = x[:, :, :, :-1]\n",
        "    return x\n",
        "\n",
        "\n",
        "def _chunk(x, w):\n",
        "    '''convert into overlapping chunkings. Chunk size = 2w, overlap size = w'''\n",
        "\n",
        "    # non-overlapping chunks of size = 2w\n",
        "    '''convert into overlapping chunkings. Chunk size = 2w, overlap size = w'''\n",
        "\n",
        "    # non-overlapping chunks of size = 2w\n",
        "    x = torch.from_numpy(x.numpy())\n",
        "    w = torch.from_numpy(w.numpy())\n",
        "    x = x.view(x.size(0), x.size(1) // (w * 2), w * 2, x.size(2))\n",
        "\n",
        "    # use `as_strided` to make the chunks overlap with an overlap size = w\n",
        "    chunk_size = list(x.size())\n",
        "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
        "\n",
        "    chunk_stride = list(x.stride())\n",
        "    chunk_stride[1] = chunk_stride[1] // 2\n",
        "    output = x.as_strided(size=chunk_size, stride=chunk_stride)\n",
        "    return tf.convert_to_tensor(output.numpy())\n",
        "\n",
        "\n",
        "def sliding_chunks_matmul_qk(q, k, w, padding_value):\n",
        "    '''Matrix multiplication of query x key tensors using with a sliding window attention pattern.\n",
        "    This implementation splits the input into overlapping chunks of size 2w (e.g. 512 for pretrained Longformer)\n",
        "    with an overlap of size w'''\n",
        "    \n",
        "    bsz, seqlen, num_heads, head_dim = tf.shape_list(q)\n",
        "    assert seqlen % (w * 2) == 0\n",
        "    assert tf.shape_list(q) == tf.shape_list(k)\n",
        "\n",
        "    chunks_count = seqlen // w - 1\n",
        "\n",
        "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n",
        "    q = tf.transpose(q, (0,2,1,3))\n",
        "    q = tf.reshape(q, (bsz * num_heads, seqlen, head_dim))\n",
        "    k = tf.transpose(k, (0,2,1,3))\n",
        "    k = tf.reshape(k, (bsz * num_heads, seqlen, head_dim))\n",
        "\n",
        "    chunk_q = _chunk(q, w)\n",
        "    chunk_k = _chunk(k, w)\n",
        "\n",
        "    chunk_attn = tf.einsum('bcxd,bcyd->bcxy', chunk_q, chunk_k)\n",
        "\n",
        "    # convert diagonals into columns\n",
        "    diagonal_chunk_attn = _skew(chunk_attn, padding_value=padding_value)\n",
        "    diagonal_chunk_attn = torch.from_numpy(diagonal_chunk_attn.numpy())\n",
        "    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n",
        "\n",
        "    # copy parts from diagonal_chunk_attn into the compined matrix of attentions\n",
        "    # - copying the main diagonal and the upper triangle\n",
        "    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n",
        "    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n",
        "    # - copying the lower triangle\n",
        "    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n",
        "    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n",
        "\n",
        "    # separate bsz and num_heads dimensions again\n",
        "    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1).transpose(2, 1)\n",
        "    #diagonal_attn = torch.from_numpy(diagonal_attn.numpy())\n",
        "    w = torch.from_numpy(w.numpy())\n",
        "    diagonal_attn = torch.from_numpy(diagonal_attn.numpy())\n",
        "    mask_invalid_locations(diagonal_attn, w, 1, False)\n",
        "    diagonal_attn = tf.convert_to_tensor(diagonal_attn.numpy())\n",
        "    return diagonal_attn\n",
        "\n",
        "\n",
        "def sliding_chunks_matmul_pv(probs, v, w):\n",
        "    '''Same as sliding_chunks_matmul_qk but for prob and value tensors. It is expecting the same output\n",
        "    format from sliding_chunks_matmul_qk'''\n",
        "    bsz, seqlen, num_heads, head_dim = tf.shape_list(v)\n",
        "    assert seqlen % (w * 2) == 0\n",
        "    assert tf.shape_list(probs)[:3] == tf.shape_list(v)[:3]\n",
        "    assert tf.shape_list(probs)[3] == 2 * w + 1\n",
        "    chunks_count = seqlen // w - 1\n",
        "    chunk_prob = tf.transpose(probs, (0,2,1,3))\n",
        "    chunk_prob = tf.reshape(chunk_prob, (bsz * num_heads, seqlen // w, w, 2 * w + 1))\n",
        "\n",
        "    v = tf.transpose(v, (0,2,1,3))\n",
        "    v = tf.reshape(v, (bsz * num_heads, seqlen, head_dim))\n",
        "\n",
        "    pads = tf.convert_to_tensor([0,0], [w,w], [0,0])\n",
        "    padded_v = tf.pad(v, pads, constant_values=-1)\n",
        "\n",
        "    # chunk padded_v into chunks of size 3w and an overlap of size w\n",
        "    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n",
        "    padded_v = torch.from_numpy(padded_v.numpy())\n",
        "    chunk_v_stride = padded_v.stride()\n",
        "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
        "    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
        "    chunk_v = tf.convert_to_tensor(chunk_v.numpy())\n",
        "\n",
        "    skewed_prob = _skew2(chunk_prob, padding_value=0)\n",
        "\n",
        "    context = tf.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n",
        "    context = tf.reshape(context, (bsz, num_heads, seqlen, head_dim))\n",
        "    return tf.transpose(context, (0,2,1,3))\n",
        "\n",
        "\n",
        "def pad_to_window_size(input_ids, attention_mask,\n",
        "                       one_sided_window_size, pad_token_id):\n",
        "    '''A helper function to pad tokens and mask to work with the sliding_chunks implementation of Longformer selfattention.\n",
        "    '''\n",
        "    w = int(2 * one_sided_window_size)\n",
        "    seqlen = tf.shape_list(input_ids)[:2]\n",
        "    padding_len = (w - seqlen % w) % w\n",
        "    pads = tf.convert_to_tensor([[0, 0], [0, padding_len]])\n",
        "    input_ids = tf.pad(input_ids, pads, constant_values=pad_token_id)\n",
        "    attention_mask = tf.pad(attention_mask, pads, constant_values=False)  # no attention on the padding tokens\n",
        "    return input_ids, attention_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NB5zUkbSXzsl",
        "outputId": "1d9ed5b1-b5e0-44f3-a693-9e565b50c264"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from typing import List\n",
        "import math\n",
        "import tensorflow as tf\n",
        "#from longformer.diagonaled_mm_tvm import diagonaled_mm as diagonaled_mm_tvm, mask_invalid_locations\n",
        "from sliding_chunks import sliding_chunks_matmul_qk, sliding_chunks_matmul_pv\n",
        "# from longformer.sliding_chunks import sliding_chunks_no_overlap_matmul_qk, sliding_chunks_no_overlap_matmul_pv\n",
        "from modeling_roberta_tf import RobertaConfig, RobertaModel, RobertaForMaskedLM, LongformerSelfAttention\n",
        "import numpy as np\n",
        "class Longformer(RobertaModel):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super(Longformer, self).__init__(config, **kwargs)\n",
        "        for i, layer in enumerate(self.encoder.layer):\n",
        "            layer.attention.self = LongformerSelfAttention(config, layer_id=i)\n",
        "\n",
        "\n",
        "class LongformerForMaskedLM(RobertaForMaskedLM):\n",
        "    def __init__(self, config, layer_id, **kwargs):\n",
        "        super(LongformerForMaskedLM, self).__init__(config, **kwargs)\n",
        "        for i, layer in enumerate(self.roberta.encoder.layer):\n",
        "            layer.attention.self = LongformerSelfAttention(config, layer_id=i)\n",
        "\n",
        "\n",
        "class LongformerConfig(RobertaConfig):\n",
        "    def __init__(self, attention_window: List[int] = None, attention_dilation: List[int] = None,\n",
        "                 autoregressive: bool = False, attention_mode: str = 'sliding_chunks', **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            attention_window: list of attention window sizes of length = number of layers.\n",
        "                window size = number of attention locations on each side.\n",
        "                For an affective window size of 512, use `attention_window=[256]*num_layers`\n",
        "                which is 256 on each side.\n",
        "            attention_dilation: list of attention dilation of length = number of layers.\n",
        "                attention dilation of `1` means no dilation.\n",
        "            autoregressive: do autoregressive attention or have attention of both sides\n",
        "            attention_mode: 'n2' for regular n^2 self-attention, 'tvm' for TVM implemenation of Longformer\n",
        "                selfattention, 'sliding_chunks' for another implementation of Longformer selfattention\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention_window = attention_window\n",
        "        self.attention_dilation = 1\n",
        "        self.autoregressive = False\n",
        "        self.attention_mode = 'sliding_chunks'\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "ziqd8QpoYEy2",
        "outputId": "cd34aba8-d573-44e4-ac6d-87e1fe3b9dfd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 1192\n",
            "})\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d74e743870d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_shard_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOFF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m tf_train_dataset = model.prepare_tf_dataset(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mprepare_tf_dataset\u001b[0;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset argument should be a datasets.Dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0mmodel_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"cols_to_retain\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_output_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             output_signature, _ = dataset._get_output_signature(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mfind_labels\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"QuestionAnswering\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"label\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"start_positions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_positions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'RobertaForMultipleChoice' has no attribute 'forward'"
          ]
        }
      ],
      "source": [
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "config = RobertaConfig(\n",
        "    #attention_window = 55,\n",
        "    vocab_size=30522,\n",
        "    hidden_size=768,\n",
        "    num_hidden_layers=12,\n",
        "    num_attention_heads=12,\n",
        "    intermediate_size=3072,\n",
        "    hidden_act=\"gelu\",\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "    initializer_range=0.02,\n",
        "    layer_norm_eps=1e-12,\n",
        "    pad_token_id=1,\n",
        "    bos_token_id=0,\n",
        "    eos_token_id=2,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        "    classifier_dropout=None,\n",
        ")\n",
        "model = RobertaForMultipleChoice(config=config)\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir= '/content/output_direc',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "print(train_dataset)\n",
        "options = tf.data.Options()\n",
        "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "\n",
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=32\n",
        ").with_options(options)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Uv7edHocbDCD",
        "CFtGA4yibG2u"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b99075048c1457081b919743250e417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "190bd36f1c3c4c4b83f8db24859b049e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_983cccbc68244182891aa5d01143862c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aff8586e7744934b0211374812de22a",
            "value": 1
          }
        },
        "1b0a4fa4e8d744e6a68dcc14dacb7833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "267d25c90073429da139fb4cee8553fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a3b5619967a4baa9786f87bc7eee365": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3132df509bb14081a3094b70f69cc0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_267d25c90073429da139fb4cee8553fa",
            "placeholder": "​",
            "style": "IPY_MODEL_964baa10d5d044e4929f84a02f615df2",
            "value": " 1/1 [00:00&lt;00:00, 32.91it/s]"
          }
        },
        "321329a721ae42a9b235fa15144a9ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d33ab7971094c6780496d915ae3c919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a3b5619967a4baa9786f87bc7eee365",
            "placeholder": "​",
            "style": "IPY_MODEL_0b99075048c1457081b919743250e417",
            "value": "100%"
          }
        },
        "41306568c3ef4365ac4a3945b5a6421f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aff8586e7744934b0211374812de22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7516c19d41de4f6cb2c8279f6fbed2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c31e59f233ab4cf3b9fc29e278e38868",
            "placeholder": "​",
            "style": "IPY_MODEL_8d5846b7f44a4c2b8db37440135b257d",
            "value": " 1/1 [00:00&lt;00:00, 36.26it/s]"
          }
        },
        "87e502ff68e54b0590eee18baf246026": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5846b7f44a4c2b8db37440135b257d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "964baa10d5d044e4929f84a02f615df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97b640e57a9b48c5ba374e86144d33c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "983cccbc68244182891aa5d01143862c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a120ea6447b0405aaf5734e5c0c2207f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e502ff68e54b0590eee18baf246026",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b0a4fa4e8d744e6a68dcc14dacb7833",
            "value": 1
          }
        },
        "b42e86fd94f645d79439a511b6638021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_321329a721ae42a9b235fa15144a9ffe",
            "placeholder": "​",
            "style": "IPY_MODEL_41306568c3ef4365ac4a3945b5a6421f",
            "value": "100%"
          }
        },
        "c31e59f233ab4cf3b9fc29e278e38868": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da3f31170f1f4000a2167fa31d7a9fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d33ab7971094c6780496d915ae3c919",
              "IPY_MODEL_a120ea6447b0405aaf5734e5c0c2207f",
              "IPY_MODEL_3132df509bb14081a3094b70f69cc0fe"
            ],
            "layout": "IPY_MODEL_f1ff8237afd845f3b184e164afc4aba5"
          }
        },
        "e2fb3e7be4144ce1b3e90da034cf1c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b42e86fd94f645d79439a511b6638021",
              "IPY_MODEL_190bd36f1c3c4c4b83f8db24859b049e",
              "IPY_MODEL_7516c19d41de4f6cb2c8279f6fbed2dc"
            ],
            "layout": "IPY_MODEL_97b640e57a9b48c5ba374e86144d33c3"
          }
        },
        "f1ff8237afd845f3b184e164afc4aba5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
